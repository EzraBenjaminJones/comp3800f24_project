{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "COMP3800 Project 3 Ezra Jones\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "#nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import re as re\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud as wc\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "\n",
    "SEED = 3800\n",
    "np.random.seed(SEED)\n",
    "sk.utils.check_random_state(SEED)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer #Porter2 Stemmer\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "__author__ = 'Ezra Jones'\n",
    "__version__ = 'Fall 2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in tweet data from HW6 with type column included\n",
    "\n",
    "df = pd.read_csv('data/comp3800f24_tweets.txt', dtype={'id': str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurate shape\n",
    "\n",
    "print(df['id'].nunique())\n",
    "print(df['id'].count())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop misaligned rows and keep only desired columns (remove type).\n",
    "\n",
    "df = df.loc[df['type'] == 'tweet']\n",
    "df.drop('type', axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset of all tweets, dataset of tweets with high polarities, and dataset of tweets with low polarities.\n",
    "\n",
    "twitter = df;\n",
    "tweets = []\n",
    "positive_tweets = []\n",
    "negative_tweets = []\n",
    "for tweet in twitter['text']:\n",
    "    tweets.append(tweet)\n",
    "    if (TextBlob(tweet).sentiment.polarity > 0.5):\n",
    "        positive_tweets.append(tweet)\n",
    "    if (TextBlob(tweet).sentiment.polarity < -0.5):\n",
    "        negative_tweets.append(tweet)\n",
    "text = ' '.join(tweets)\n",
    "negative_text = ' '.join(negative_tweets)\n",
    "positive_text = ' '.join(positive_tweets)\n",
    "f'one big text:\\n{text}';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize tweets\n",
    "\n",
    "tokens = nltk.word_tokenize(text, language='english', preserve_line=True)\n",
    "negative_tokens = nltk.word_tokenize(negative_text, language='english', preserve_line=True)\n",
    "positive_tokens = nltk.word_tokenize(positive_text, language='english', preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "\n",
    "words = [re.sub(r'[^A-Za-z0-9]+', '', token) for token in tokens]\n",
    "words = [word for word in words if word]\n",
    "negative_words = [re.sub(r'[^A-Za-z0-9]+', '', token) for token in negative_tokens]\n",
    "negative_words = [word for word in negative_words if word]\n",
    "positive_words = [re.sub(r'[^A-Za-z0-9]+', '', token) for token in positive_tokens]\n",
    "positive_words = [word for word in positive_words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and convert words to lower\n",
    "\n",
    "stops = set(stopwords.words('english')) \n",
    "terms = [word for word in words if word.lower() not in stops]\n",
    "negative_terms = [word for word in negative_words if word.lower() not in stops]\n",
    "positive_terms = [word for word in positive_words if word.lower() not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stems, (removing https).\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stems = [stemmer.stem(word) for word in terms if word != 'https']\n",
    "negative_stems = [stemmer.stem(word) for word in negative_terms if word != 'https']\n",
    "positive_stems = [stemmer.stem(word) for word in positive_terms if word != 'https']\n",
    "stem_freq_dist = nltk.FreqDist(stems)\n",
    "df_words = pd.DataFrame(list(stem_freq_dist.items()), columns = ['stem','freq'])\n",
    "df_sorted = df_words.sort_values(by=['freq'], ascending=False)\n",
    "\n",
    "df_sorted = df_sorted[df_sorted['stem'] != 'https']\n",
    "\n",
    "# Bar chart visualizing the frequency of the top 20 most used words/stems.\n",
    "\n",
    "freq = df_sorted.head(20)\n",
    "plt.bar(freq[\"stem\"], freq[\"freq\"])\n",
    "plt.xlabel('Stem')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud showing most frequently used word stems in top negative tweets\n",
    "\n",
    "negative_stem_freq_dist = nltk.FreqDist(negative_stems)\n",
    "wordcloud = wc.WordCloud(width=1920, height=1080).generate_from_frequencies(dict(negative_stem_freq_dist))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud showing most frequently used word stems in top positive tweets\n",
    "\n",
    "positive_stem_freq_dist = nltk.FreqDist(positive_stems)\n",
    "wordcloud = wc.WordCloud(width=1920, height=1080).generate_from_frequencies(dict(positive_stem_freq_dist))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiments of the most retweeted tweets (25000 retweets or greater)\n",
    "\n",
    "popular_df = twitter[pd.to_numeric(twitter['retweetCount'], errors='coerce') > 25000]\n",
    "popular_tweets = []\n",
    "for tweet in popular_df['text']:\n",
    "    popular_tweets.append(tweet)\n",
    "\n",
    "sentiments = [TextBlob(tweet).sentiment for tweet in popular_tweets]\n",
    "polarities = [sentiment.polarity for sentiment in sentiments]\n",
    "subjectivities = [sentiment.subjectivity for sentiment in sentiments]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if polarity > 0 else 'blue' for polarity in polarities]\n",
    "sizes = [1000 * abs(polarity) for polarity in polarities]\n",
    "plt.scatter(polarities, subjectivities, c=colors, s=sizes, alpha=0.5)\n",
    "\n",
    "for i, txt in enumerate(popular_df['retweetCount']):\n",
    "    plt.annotate(txt, (polarities[i], subjectivities[i]), fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.title('Sentiment Analysis of Popular Tweets (25K+ retweets)')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f'Average Polarity: {np.mean(polarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep dataset for polarity time function, dropping tweets that don't contain content relevant to \"ai\".\n",
    "\n",
    "df['createdAt'] = pd.to_datetime(df['createdAt'], errors='coerce')\n",
    "df['Date'] = df['createdAt'].dt.date\n",
    "df = df.loc[df['Date'] >= pd.to_datetime('2020-01-01').date()]\n",
    "df['text'] = df['text'].str.lower()\n",
    "df = df.loc[df['text'].str.contains('ai')]\n",
    "\n",
    "popular_tweets = []\n",
    "for tweet in df['text']:\n",
    "    popular_tweets.append(tweet)\n",
    "\n",
    "sentiments = [TextBlob(tweet).sentiment for tweet in popular_tweets]\n",
    "polarities = [sentiment.polarity for sentiment in sentiments]\n",
    "\n",
    "df['Polarity'] = polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure for time function. Showing sentiment before, during, and after buyout and terms of service change dates.\n",
    "\n",
    "buyout_day = '2022-4-14'\n",
    "ai_train_day = '2024-10-15'\n",
    "before_buyout_sentiment = df.loc[df['Date'] < pd.to_datetime(buyout_day).date(), 'Polarity'].mean()\n",
    "before_buyout_count = df.loc[df['Date'] < pd.to_datetime(buyout_day).date()]['id'].count()\n",
    "before_buyout_message = f'Before Buyout: {before_buyout_sentiment:.2f} ({before_buyout_count}) tweets'\n",
    "\n",
    "after_buyout_sentiment = df.loc[(df['Date'] >= pd.to_datetime(buyout_day).date()) & (df['Date'] < pd.to_datetime(ai_train_day).date()), 'Polarity'].mean()\n",
    "after_buyout_count = df.loc[(df['Date'] >= pd.to_datetime(buyout_day).date()) & (df['Date'] < pd.to_datetime(ai_train_day).date())]['id'].count()\n",
    "after_buyout_message = f'After Buyout, Before Announcement: {after_buyout_sentiment:.2f} ({after_buyout_count}) tweets'\n",
    "\n",
    "after_announcement_sentiment = df.loc[df['Date'] >= pd.to_datetime(ai_train_day).date(), 'Polarity'].mean()\n",
    "after_announcement_count = df.loc[df['Date'] >= pd.to_datetime(ai_train_day).date()]['id'].count()\n",
    "after_announcement_message = f'After Announcement: {after_announcement_sentiment:.2f} ({after_announcement_count}) tweets'\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.lineplot(x='Date', y='Polarity', data=df)\n",
    "plt.text(pd.to_datetime('2020-01-01').date(),1.1,before_buyout_message,rotation=0, horizontalalignment='center')\n",
    "grok_date = pd.to_datetime(ai_train_day)\n",
    "plt.axvline(x=grok_date, color='red', linestyle='--')\n",
    "plt.text(grok_date,1.1,after_announcement_message,rotation=0, horizontalalignment='center')\n",
    "X_date = pd.to_datetime(buyout_day)\n",
    "plt.axvline(x=X_date, color='red', linestyle='--')\n",
    "plt.text(X_date,1.1,after_buyout_message,rotation=0, horizontalalignment='center')\n",
    "plt.title('Sentiment over Time', y=1.1)\n",
    "plt.ylabel('Polarity Score', fontsize=12)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe from main twitter df, keeping text column for word2vec vectorization. Perform word2vec vectorization on\n",
    "# text column, and reduce dimensionality with PCA. Add PCA columns to new dataframe and define sentiments for all tweets.\n",
    "\n",
    "w2v_df = twitter[['text']]\n",
    "w2v_df['tokenized_text'] = w2v_df['text'].apply(lambda x: x.lower().split())\n",
    "corpus = w2v_df['tokenized_text'].tolist()\n",
    "\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "def get_vector(text):\n",
    "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)  # Average word vectors\n",
    "    else:\n",
    "        return [0] * model.vector_size  # Return a vector of zeros if no word is found\n",
    "\n",
    "w2v_df['word2vec'] = w2v_df['tokenized_text'].apply(get_vector)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca_result = pca.fit_transform(w2v_df['word2vec'].tolist())\n",
    "w2v_df['pca_1'] = pca_result[:, 0]\n",
    "w2v_df['pca_2'] = pca_result[:, 1]\n",
    "w2v_df['pca_3'] = pca_result[:, 2]\n",
    "w2v_df['pca_4'] = pca_result[:, 3]\n",
    "w2v_df['pca_5'] = pca_result[:, 4]\n",
    "w2v_df['pca_6'] = pca_result[:, 5]\n",
    "w2v_df['pca_7'] = pca_result[:, 6]\n",
    "w2v_df['pca_8'] = pca_result[:, 7]\n",
    "w2v_df['pca_9'] = pca_result[:, 8]\n",
    "w2v_df['pca_10'] = pca_result[:, 9]\n",
    "w2v_df.drop('text', axis=1, inplace=True)\n",
    "w2v_df.drop('tokenized_text', axis=1, inplace=True)\n",
    "w2v_df.drop('word2vec', axis=1, inplace=True)\n",
    "\n",
    "sentiments = [TextBlob(tweet).sentiment for tweet in tweets]\n",
    "polarities = [sentiment.polarity for sentiment in sentiments]\n",
    "subjectivities = [sentiment.subjectivity for sentiment in sentiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to new df containing values for each tweet's subjectivity and polarity. Define Subjectivity as the target feature for the model,\n",
    "# and assign all pca components and tweet polarity as input. Create train-test split.\n",
    "\n",
    "w2v_df['Subjectivity'] = subjectivities\n",
    "w2v_df['Polarity'] = polarities\n",
    "\n",
    "target_label = 'Subjectivity'\n",
    "feature1 = 'pca_1'\n",
    "feature2 = 'pca_2'\n",
    "feature3 = 'pca_3'\n",
    "feature4 = 'pca_4'\n",
    "feature5 = 'pca_5'\n",
    "feature6 = 'pca_6'\n",
    "feature7 = 'pca_7'\n",
    "feature8 = 'pca_8'\n",
    "feature9 = 'pca_9'\n",
    "feature10 = 'pca_10'\n",
    "feature11 = 'Polarity'\n",
    "features = [feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, feature11]\n",
    "\n",
    "\n",
    "X = w2v_df[features]\n",
    "y = w2v_df[target_label]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, print evaluation metrics, and visualize a graph detailing performance by comparing predicted values to \n",
    "# actual values. Model appears to be underfitting somewhat, unable to capture the relationship between inputs for some tweets.\n",
    "# However, even in this state, the model is performing moderately well, with an R2 score of 0.54~. The graph loosley shows\n",
    "# that as the actual value increases, the majority of the predicted values increase as well.\n",
    "\n",
    "model = MLPRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "df_results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "df_results.plot(kind='scatter', x='Actual', y='Predicted')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'R-squared: {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
